# HW 14

## 2. Document the micro-service architecture and components/tools/dependencies.

![image-20230528153224403](/Users/zhuhaotian/Library/Application Support/typora-user-images/image-20230528153224403.png)

1. API Gateway:
   * It acts as a single entry point for all external client requests and provides a unified interface to access the various microservices within the system.
   * Zuul API Gateway,Spring Cloud Gateway
2. Load Balancer:
   * A critical component that helps distribute incoming network traffic across multiple instances of a microservice to ensure optimal resource utilization, improved performance, and high availability. It acts as a traffic cop, distributing requests in a balanced manner to different instances of a microservice based on predefined algorithms or rules.
   * Ribbon
3. Circuit Breaker:
   * A circuit breaker is a design pattern that helps manage failures and prevent cascading failures between services. It acts as a safety mechanism to protect the system from prolonged downtime or performance degradation caused by unresponsive or failing microservices. The circuit breaker pattern is particularly useful in distributed systems where services can be unreliable or experience intermittent failures. 
   * Hystrix, Resilience4J, Spring Retry
4. Service Registry:
   * A service registry is a component that enables service discovery and facilitates communication between microservices. It acts as a centralized repository where microservices can register themselves and provide information about their location, availability, and endpoints.
   * Eureka, Nacos
5. Message Queue:
   * A message queue is a fundamental component used for asynchronous communication and decoupling of microservices. It provides a reliable and scalable way to exchange messages or events between different services.
   * Kafka, RabbitMQ, RocketMQ

6. Distributed Configuration:
   * Distributed configuration refers to the practice of managing configuration settings and parameters for individual microservices in a decentralized manner. Instead of relying on a centralized configuration management system, each microservice manages its own configuration independently. 
   * Spring Cloud Config Server, Nacos

## 3. What is Resilience Patterns? What is circuit Breaker?

Resilience patterns are architectural and design principles that help systems withstand and recover from failures, disruptions, and unexpected conditions. These patterns aim to build robust and reliable systems that can adapt to changing circumstances, prevent cascading failures, and provide a satisfactory level of service even in the face of adverse events. Resilience patterns are often employed in distributed systems, including microservice architectures. 

A circuit breaker is a design pattern that helps manage failures and prevent cascading failures between services. It acts as a safety mechanism to protect the system from prolonged downtime or performance degradation caused by unresponsive or failing microservices. The circuit breaker pattern is particularly useful in distributed systems where services can be unreliable or experience intermittent failures. 

## 4. Read this article, then list the important questions, then write your answers.

1. Write main features of Microservices:

- **Decoupling**: Within a system, services are largely decoupled. The application as a whole can therefore be easily constructed, altered, and scalable
- **Componentization**: Microservices are viewed as independent components that can easily be exchanged or upgraded
- **Business Capabilities**: Microservices are relatively simple and only focus on one service
- **Team autonomy**: Each developer works independently of each other, allowing for a faster project timeline
- **Continuous Delivery**: Enables frequent software releases through systematic automation of software development, testing, and approval
- **Responsibility:** Microservices are not focused on applications as projects. Rather, they see applications as products they are responsible for
- **Decentralized Governance:** Choosing the right tool according to the job is the goal. Developers can choose the best tools to 

2. Write difference between Monolithic, SOA and Microservices Architecture.

- **Monolithic Architecture**: It is "like a big container" where all the software components of an application are bundled together tightly.  It is usually built as one large system and is one code-base. 
- **SOA (Service-Oriented Architecture)**: It is a group of services interacting or communicating with each other. Depending on the nature of the communication, it can be simple data exchange or it could involve several services coordinating some activity.  
- **Microservice Architecture**: It involves structuring an application in the form of a cluster of small, autonomous services modeled around a business domain. The functional modules can be deployed independently, are scalable, are aimed at achieving specific business goals, and communicate with each other over standard protocols. 

3. What do you mean by Cohesion and Coupling?

**Coupling**: It is defined as a relationship between software modules A and B, and how much one module depends or interacts with another one. Couplings fall into three major categories. Modules can be highly coupled (highly dependent), loosely coupled, and uncoupled from each other. The best kind of coupling is loose coupling, which is achieved through interfaces. 

**Cohesion**: It is defined as a relationship between two or more parts/elements of a module that serves the same purpose. Generally, a module with high cohesion can perform a specific function efficiently without needing communication with any other modules. High cohesion enhances the functionality of the module.

4. Write the fundamental characteristics of Microservice Design.

- **Based on Business Capabilities**: Services are divided and organized around business capabilities. 
- **Products not projects:** A product should belong to the team that handles it.  
- **Essential messaging frameworks:** Rely on functional messaging frameworks: Eliminate centralized service buses by embracing the concept of decentralization.  
- **Decentralized Governance:** The development teams are accountable for all aspects of the software they produce.  
- **Decentralized data management:** Microservices allow each service to manage its data separately.  
- **Automated infrastructure:** These systems are complete and can be deployed independently.  
- **Design for failure:** Increase the tolerance for failure of services by focusing on continuous monitoring of the applications. 

5. Explain PACT in microservices.

   PACT stands for "Producer-Consumer Contract Testing." PACT is defined as an open-source tool that allows service providers and consumers to test interactions in isolation against contracts that have been made to increase the reliability of microservice integration. It also offers support for numerous languages, such as Ruby, Java, Scala, .NET, JavaScript, Swift/Objective-C. 

6. Explain how independent microservices communicate with each other.

   Communication between microservices can take place through: 

   - HTTP/REST with JSON or binary protocol for request-response 
   - Websockets for streaming.  
   - A broker or server program that uses advanced routing algorithms.  

   RabbitMQ, Nats, Kafka, etc., can be used as message brokers; each is built to handle a particular message semantic. You can also use Backend as a Service like Space Cloud to automate your entire backend.

7. Explain CDC.

   As the name implies, CDC (Consumer-Driven Contract) basically ensures service communication compatibility by establishing an agreement between consumers and service providers regarding the format of the data exchanged between them. An agreement like this is called a contract. Basically, it is a pattern used to develop Microservices so that they can be efficiently used by external systems.

8. What do you mean by Domain driven design?

   DDD (Domain-Driven-Design) is basically an architectural style that is based on Object-Oriented Analysis Design approaches and principles. In this approach, the business domain is modeled carefully in software, without regard to how the system actually works. By interconnecting related components of the software system into a continuously evolving system, it facilitates the development of complex systems. There are three fundamental principles underlying it as shown below: 

   - Concentrate on the core domain and domain logic. 
   - Analyze domain models to find complex designs. 
   - Engage in regular collaboration with the domain experts to improve the application model and address emerging domain issues. 

## 5. How to do load balance in microservice? Write a long Summary by yourself.

A load balancer routes client requests across all servers. It simply distributes the set of requested operations effectively across multiple servers and ensures that no single server bears too many requests that lead to degrading the overall performance of the application.

There are two main problems with web application: single point of failure and overloaded servers. We can use load balancer to solve these two problems.

- Load balancers minimize server response time and maximize throughput.
- Load balancer ensures high availability and reliability by sending requests only to online servers
- Load balancers do continuous health checks to monitor the serverâ€™s capability of handling the request.
- Depending on the number of requests or demand load balancers add or remove the number of servers.

Where are load balancers typically placed?

![Load-Balancer-Placed](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20201030211106/Load-Balancer-Placed.png)

Types of load balancers: Software load balancers in clients; Software load balancers in services; Hardware load balancers.

Categories of load balancers:  L4 load balancer; L7 load balancer; Global Server Load Balancing(GSLB).

Load balancing algorithms: Round Robin (Simple to implement); Weighted Round Robin; Least Connection Method; Least Response Time Method; Source IP Hash.

## 6. How to do service discovery?

![image-20230529024444610](/Users/zhuhaotian/Library/Application Support/typora-user-images/image-20230529024444610.png)

1. **Client-side service discovery**: In this approach, each service instance **is responsible for registering itself with a service registry and updating its status periodically**.
2. **Server-side service discovery**: In this approach, **the service registry acts as a central authority responsible for maintaining a record of all registered services and their network locations**.
3. **DNS-based service discovery**: **Each service registers itself with a DNS server, which maps a service name to its network location.** 
4. **Service mesh**: **A service mesh is an infrastructure layer that handles service-to-service communication within a microservices architecture.** It typically incorporates service discovery as a core feature.

## 7. What are the major components of Kafka?

1. **Topic**: A topic is a category or feed name to which messages are published by producers and from which consumers consume messages. Topics are divided into partitions for parallel processing and can be replicated across multiple brokers for fault tolerance.
2. **Producer**: A producer is a client application or system that publishes messages to Kafka topics. It writes messages to a specific topic and can choose to publish messages synchronously or asynchronously.
3. **Consumer**: A consumer is a client application or system that subscribes to one or more topics and consumes messages from Kafka. Consumers read messages in the order they were written to the topic and can choose to read from specific partitions or consume from all partitions.
4. **Broker**: A broker is a Kafka server that stores and manages the streams of records (messages). It is responsible for handling producer and consumer requests, replicating data across multiple brokers for fault tolerance, and ensuring proper distribution of partitions.
5. **Partition**: A topic can be divided into multiple partitions, which are ordered and immutable sequences of records. Each partition is hosted on a specific broker and can be spread across multiple brokers for scalability. Partitioning enables parallel processing and allows for higher throughput and fault tolerance.
6. **Offset**: An offset is a unique identifier assigned to each message within a partition. It represents the position of a consumer in the partition's message stream and enables precise control over the consumed messages.
7. **Consumer Group**: A consumer group is a group of consumer instances that work together to consume messages from a topic. Each consumer in a group is assigned one or more partitions, ensuring that each partition is consumed by only one consumer at a time. Consumer groups enable parallel processing and scale the consumption of messages.
8. **ZooKeeper**: Although not strictly a part of Kafka, ZooKeeper is a separate component often used alongside Kafka. ZooKeeper is a distributed coordination service that helps manage and maintain configuration, synchronization, and leader election for Kafka brokers.

## 8. What do you mean by a Partition in Kafka?

A partition is a fundamental concept that represents a subset of a topic's data. A topic can be divided into multiple partitions, and each partition is an ordered and immutable sequence of records (messages). Partitions allow data to be split across multiple brokers in a Kafka cluster, enabling scalability, parallel processing, and fault tolerance.

## 9. What do you mean by zookeeper in Kafka and what are its uses?


ZooKeeper is a separate component that is often used in conjunction with Apache Kafka. It is a distributed coordination service that provides functionalities such as configuration management, synchronization, and distributed consensus for distributed systems like Kafka. Here's an overview of ZooKeeper and its uses in the Kafka ecosystem:

1. **Configuration Management**: ZooKeeper serves as a centralized repository for storing and managing configuration information of a Kafka cluster. It stores essential metadata such as broker addresses, topic configurations, and consumer group information. Kafka brokers and clients can connect to ZooKeeper to retrieve and update this configuration data, allowing for dynamic cluster management.
2. **Leader Election**: Kafka relies on ZooKeeper for leader election. Each partition in a Kafka topic has one leader that handles read and write requests, while other replicas serve as followers. ZooKeeper helps ensure that a consistent leader is elected for each partition, even in the presence of failures or changes in the cluster. ZooKeeper watches and notifies Kafka brokers about leadership changes, enabling them to maintain a coordinated state.
3. **Brokers Registration**: Kafka brokers register themselves with ZooKeeper upon startup. This registration allows new brokers to join the cluster and existing brokers to update their status, metadata, and availability information. Consumers and producers can then discover the current list of available brokers by querying ZooKeeper, simplifying the process of connecting to the Kafka cluster.
4. **Topic Partition Management**: ZooKeeper tracks the state and assignment of partitions within Kafka topics. It maintains information about which brokers host which partitions and tracks the health and status of these partitions. This data is crucial for Kafka's partition distribution, fault tolerance, and rebalancing mechanisms.
5. **Offset Tracking**: ZooKeeper can be used to store and manage consumer offsets. Consumer applications can store the offset of the last message they have processed for each partition in ZooKeeper. This allows consumers to resume reading from the correct position after failures or restarts.
6. **Watchers and Notifications**: ZooKeeper provides a mechanism called "watchers" that allows Kafka brokers and clients to be notified of changes in the cluster's state. For example, when a new topic is created, a topic configuration is modified, or a broker goes offline, ZooKeeper notifies the interested parties, enabling them to react and update their internal state accordingly.

## 10. Can we use Kafka without Zookeeper?

Yes, after Kafka 3.3, ZooKeeper can be replaced by Kraft and after Kafka 4.0, ZooKeeper is no longer supported.

## 11. Explain the concept of Leader and Follower in Kafka.

**Leader**:

- Each partition in a Kafka topic has one leader.
- The leader is responsible for handling all read and write requests for that partition.
- Producers send messages to the leader, and consumers read messages from the leader.
- The leader maintains an in-sync replica (ISR) set, which consists of all replicas that are caught up with the leader's data. These replicas are considered "in sync" and participate in the replication process.
- The leader is the only replica that can accept and acknowledge write requests. It ensures that writes are appended in the correct order and guarantees strong consistency within the partition.

**Follower**:

- Followers are replicas of a partition that replicate data from the leader.
- They maintain a copy of the leader's data by continuously fetching and applying the log of messages from the leader.
- Followers stay in sync with the leader by sending periodic fetch requests to the leader, which contain the latest offset they have replicated.
- Followers are responsible for providing read scalability and fault tolerance. Consumers can read from followers to distribute the read load and increase overall throughput.
- If the leader fails, one of the followers is selected as the new leader through leader election, ensuring continuous availability and data durability.

## 12. Why is Topic Replication important in Kafka? What do you mean by ISR in Kafka?

Topic replication is a critical feature in Apache Kafka that provides data durability, fault tolerance, and high availability.

The In-Sync Replica (ISR) is a concept associated with topic replication. ISR represents the set of replicas that are considered "in sync" with the leader replica. The ISR ensures that replicas are sufficiently up-to-date with the leader's data to maintain consistency and provide durability guarantees. Replicas that are not up-to-date with the leader are considered out-of-sync (OSR).

## 13. What do you understand about a consumer group in Kafka?

In Apache Kafka, a consumer group is a logical grouping of consumer instances that work together to consume and process messages from one or more topics. Consumer groups provide several benefits in distributed data processing scenarios. 

## 14. How do you start a Kafka server?

1. **Download Kafka**: Visit the Apache Kafka website (https://kafka.apache.org/downloads) and download the desired version of Kafka based on your operating system.
2. **Extract the Kafka archive**: Once the download is complete, extract the Kafka archive to a directory of your choice.
3. **Configure Kafka**: Navigate to the extracted Kafka directory and locate the `config` subdirectory. Inside it, you'll find several configuration files, including `server.properties`, which is the main configuration file for the Kafka server.
   - Open the `server.properties` file and make any necessary changes, such as modifying the broker ID, listener settings, log directories, and ZooKeeper connection details. You can customize these settings based on your requirements.
4. **Start ZooKeeper**: Kafka relies on ZooKeeper for coordination, so before starting Kafka, you need to start a ZooKeeper server if you don't already have one running. Refer to the ZooKeeper documentation for instructions on starting a ZooKeeper server.
5. **Start the Kafka server**: Open a terminal or command prompt and navigate to the Kafka directory.

## 15. Tell me about some of the real-world usages of Apache Kafka.

Event streaming, data processing, data integration, server communication, and building business applications / microservices.

## 16. Describe partitioning key in Kafka.

In Kafka, a partitioning key is a field or value associated with a message that is used to determine the target partition to which the message will be written. When producing messages to a Kafka topic, you can specify a partitioning key as part of the message metadata. The partitioning key is crucial for determining the partition assignment and the order in which messages are written to a topic.

## 17. What is the purpose of partitions in Kafka?

Partitions serve several important purposes in Apache Kafka. They are a fundamental concept that plays a crucial role in the architecture and functionality of Kafka. 

## 18. Differentiate between Rabbitmq and Kafka.

1. **Messaging Model**: RabbitMQ is based on the Advanced Message Queuing Protocol (AMQP) and follows a traditional message queuing model. It focuses on reliable message delivery, queuing, and routing. Kafka, on the other hand, follows a distributed streaming platform model that emphasizes real-time data streaming and event processing. It provides a highly scalable and fault-tolerant architecture for handling high-volume, high-throughput data streams.
2. **Data Persistence**: RabbitMQ stores messages in queues, which can be durable or transient. It ensures that messages are reliably delivered and consumed by consumers. Kafka, however, uses a distributed commit log architecture and stores messages in append-only logs on disk. Kafka's design enables high durability, fault tolerance, and allows for long-term retention of large volumes of data.
3. **Message Delivery Semantics**: RabbitMQ guarantees "at least once" delivery semantics, where messages are delivered to consumers at least once but may be duplicated in case of failures. Kafka guarantees "at least once" semantics by default, but with proper consumer configuration, it can provide "exactly once" semantics for message processing, ensuring no duplication or loss.
4. **Use Cases**: RabbitMQ is well-suited for traditional messaging scenarios, such as task queuing, work distribution, request/response patterns, and reliable messaging between applications. It is often used in enterprise applications and scenarios where strict ordering and message acknowledgement are critical. Kafka, on the other hand, is designed for high-throughput, real-time data streaming and event-driven architectures. It is commonly used for log aggregation, event sourcing, data pipelines, stream processing, and building scalable, fault-tolerant systems.
5. **Message Retention and Scalability**: RabbitMQ typically retains messages until they are consumed or expired. It can handle moderate message volumes and is suitable for scenarios where data needs to be processed immediately. Kafka, with its distributed commit log, allows for long-term retention of messages. It scales horizontally by partitioning data across multiple brokers and can handle extremely high message throughput and large data volumes.
6. **Ecosystem and Integrations**: RabbitMQ has a rich ecosystem with support for various programming languages and frameworks. It provides a wide range of plugins and integrations for different messaging patterns and protocols. Kafka, on the other hand, has a strong ecosystem and is widely used for stream processing with tools like Apache Spark, Apache Flink, and Kafka Streams. It also integrates well with other components of the Apache Kafka ecosystem, such as Kafka Connect for data integration and Kafka MirrorMaker for data replication.

## 19. What are the guarantees that Kafka provides?

1. **Fault Tolerance**: Kafka is designed to be highly fault-tolerant. It replicates data across multiple brokers, ensuring that even if a broker or disk fails, data remains accessible and the system continues to operate without data loss. The leader-follower replication model allows for automatic failover, where one of the follower replicas can be promoted as the new leader in case of a leader failure.
2. **Scalability**: Kafka is built for horizontal scalability. By distributing data across multiple partitions and brokers, Kafka can handle high message throughput and large data volumes. The addition of more brokers allows for the expansion of the system's capacity, allowing it to scale as the data and workload grow.
3. **Durability**: Kafka provides durability guarantees for messages. Once messages are written to a topic and acknowledged by the leader replica, they are durably stored and retained based on the configured retention policy. Kafka ensures that messages are not lost even in the event of system failures, making it suitable for long-term storage and processing of data.
4. **Ordering**: Kafka guarantees message ordering within a partition. Messages with the same partitioning key or related messages are written and processed in the order they were produced. This strict ordering ensures that data integrity is maintained, allowing for consistent and sequential processing within a partition.
5. **Exactly Once Semantics**: Kafka supports "exactly once" semantics for message processing when properly configured. By leveraging transactional producer and consumer APIs, combined with idempotent producer configurations and appropriate consumer acknowledgment settings, Kafka allows for end-to-end processing guarantees without duplicating or losing messages.
6. **Message Retention**: Kafka allows for configurable retention policies for topics. Messages can be retained in Kafka for a specified period or based on the size of the log. This flexibility allows you to control how long data is retained in the system, making it suitable for various use cases with different data retention requirements.
7. **At Least Once Delivery**: Kafka provides "at least once" message delivery semantics. This means that messages are guaranteed to be delivered to consumers at least once, although there may be cases of duplication during failures or rebalancing. With proper configuration and acknowledgment, duplicates can be effectively handled.

## 20. What do you mean by an unbalanced cluster in Kafka? How can you balance it?

An unbalanced cluster in Kafka refers to a situation where the distribution of partitions across brokers is uneven, leading to potential performance issues and resource underutilization. In an unbalanced cluster, some brokers may be overwhelmed with a higher number of partitions, while others have a significantly lower load.

Balancing a Kafka cluster involves redistributing the partitions across brokers to achieve an even distribution and maximize resource utilization. Here are some approaches to balance an unbalanced cluster:

1. **Reassign Partitions**: Kafka provides a built-in tool called `kafka-reassign-partitions.sh` (or `kafka-reassign-partitions.bat` for Windows) that allows you to manually reassign partitions. This tool generates a JSON file that defines the new partition assignments. You can use this tool to redistribute partitions among brokers and balance the cluster. However, it's important to exercise caution when performing manual reassignments to avoid disrupting ongoing data processing or introducing data inconsistency.
2. **Automatic Rebalancing**: In some cases, Kafka's automatic partition assignment algorithm can rebalance the cluster. This occurs when new brokers are added, existing brokers recover from failures, or the number of replicas is modified. Kafka's partition assignment algorithm attempts to distribute the partitions evenly based on the number of available brokers and replicas.
3. **Capacity Planning**: Proper capacity planning is essential to prevent cluster imbalances. When provisioning new brokers, consider the desired workload and data volume. Having an appropriate number of brokers and carefully defining the replication factor can help distribute partitions more evenly across the cluster.
4. **Monitoring and Scaling**: Continuously monitor the cluster and its resource utilization. Use Kafka monitoring tools to gather metrics on broker and partition performance. If you identify any imbalances, consider scaling the cluster by adding more brokers or adjusting the number of partitions to achieve better distribution.

## 21. In your recent project, are you a producer or consumer or both?

Producer and consumer. In my Spring Cloud project, some services need to use information from other services. Kafka can be used to realize the communication between different communications, then the producer service will provide specific information to the consumer service.

## 22. In your recent project, Could you tell me your topic name?

users.registrations; users.orders; users transactions; users.shipping; users.returns.

clickstream.pageviews; clickstream.interactions; clickstream.impressions; clickstream.performance; clickstream.search.

trucks.location; trucks.drivers; trucks.status; trucks.position; trucks.routes.

## 23. In your recent project, How many brokers do you have? How many partitions for each topic? How many data for each topic.

Brokers: 3 (big clusters can have over 100 brokers).

Partitions: 5 - 10 partitions.

## 24. In your recent project, which team produce what kind of event to you and you producer what kind of events?

## 25. What is Offset?

Only have a meaning for a specific partition. To record the index of data in this partition that has been consumed.

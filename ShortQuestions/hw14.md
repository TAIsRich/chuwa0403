## 2. Document the microservice architeture and components/tools/dependencies
Components of Microservice Architecture:

Services: Microservices are the core components of the architecture. Each service represents a distinct business capability or function and is responsible for performing a specific task. Services communicate with each other through well-defined APIs, typically using lightweight protocols like HTTP/REST or messaging systems.

Service Registry and Discovery: In a microservices architecture, services need a way to discover and communicate with each other. Service registry and discovery tools like Netflix Eureka, Consul, or ZooKeeper provide a central registry where services can register themselves and discover other services dynamically.

API Gateway: An API gateway acts as a single entry point for client applications to interact with the microservices. It handles requests from clients, performs authentication and authorization, and routes the requests to the appropriate microservice. API gateways like Spring Cloud Gateway or Netflix Zuul provide routing, load balancing, and security features.

Event-Driven Communication: Microservices often communicate through asynchronous messaging patterns using message brokers like RabbitMQ or Apache Kafka. Events are published to the message broker, and other services can subscribe to those events and react accordingly. This decouples services and allows for loose coupling and scalability.

Database per Service: Each microservice can have its own dedicated database or share a common database depending on the requirements. Microservices typically use lightweight and scalable databases like MongoDB or Apache Cassandra.

Containerization: Containerization technologies like Docker provide a way to package microservices and their dependencies into lightweight and portable containers. Containers make it easier to deploy and scale microservices consistently across different environments.

Orchestration: Tools like Kubernetes provide orchestration capabilities to manage the deployment, scaling, and monitoring of microservices. They handle tasks such as service discovery, load balancing, self-healing, and rolling updates.

Monitoring and Observability: Monitoring and observability tools like Prometheus, Grafana, or ELK stack are used to collect and analyze logs, metrics, and traces from microservices. These tools help in identifying and troubleshooting performance issues and maintaining the health of the system.

Circuit Breaker: Circuit breaker patterns, implemented using libraries like Hystrix, help in managing failures and providing resilience in a distributed system. Circuit breakers monitor the health of services and prevent cascading failures by failing fast and providing fallback mechanisms.

Distributed Tracing: Distributed tracing tools like Zipkin or Jaeger enable tracking requests as they flow through multiple microservices. They provide insights into the end-to-end flow, latency, and performance of requests, helping in identifying bottlenecks and optimizing the system.

Dependencies and Tools:

Building microservices often involves using various tools, frameworks, and dependencies. Some commonly used ones include:

Spring Boot: A popular Java framework for building microservices, providing essential features like dependency injection, configuration management, and web development.

Netflix OSS: A set of open-source tools from Netflix, including Eureka for service discovery, Ribbon for client-side load balancing, Hystrix for fault tolerance, and Zuul for API gateway.

Apache Kafka: A distributed streaming platform used for building event-driven architectures and handling high-throughput, fault-tolerant messaging.

RabbitMQ: A messaging broker that implements the Advanced Message Queuing Protocol (AMQP) and is often used for asynchronous communication between microservices.

MongoDB: A NoSQL database that provides high scalability and flexibility, making it suitable for microservices that require a flexible data model.

Docker: A containerization platform that enables packaging applications and their dependencies into lightweight and portable containers.

Kubernetes: An open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.

Prometheus: A monitoring and alerting toolkit that collects metrics from microservices and provides a flexible query language for analyzing and visualizing the data.

Grafana: A data visualization tool commonly used with Prometheus to create dashboards and visual representations of metrics.

## 3. What are Resilience patterns? What is circuit breaker?
Resilience patterns, also known as resilience engineering or fault-tolerant patterns, are design principles and strategies used to build robust and reliable systems that can withstand failures and recover gracefully. These patterns help ensure that a system remains available, responsive, and functional even when faced with errors, failures, or high loads.

The Circuit Breaker pattern works by monitoring the state of the target component or service. If the target component is experiencing failures or becoming unresponsive, the circuit breaker trips and starts to short-circuit subsequent requests, preventing them from reaching the failing component. Instead, the circuit breaker returns a predetermined default response or an error to the caller.

## 4. Read this article, then list the important questions, then write your answers
### 1. Write main components of Microservices.

1. Containers, Clustering, and Orchestration 
2. IaC [Infrastructure as Code Conception] 
3. Cloud Infrastructure 
4. API Gateway 
5. Enterprise Service Bus 
6. Service Delivery 

### 2. What are the benefits and drawbacks of Microservices?
Benefits: 
1. Self-contained, and independent deployment module. 
2. Independently managed services.   
3. In order to improve performance, the demand service can be deployed on multiple servers.   
4. It is easier to test and has fewer dependencies.  

Drawbacks: 
1. Due to the complexity of the architecture, testing and monitoring are more difficult.  
2. Lacks the proper corporate culture for it to work.   
3. Pre-planning is essential.  
4. Complex development.  

### 3. Write difference between Monolithic, SOA and Microservices Architecture.

1. Monolithic Architecture: It is "like a big container" where all the software components of an application are bundled together tightly.  It is usually built as one large system and is one code-base. 
2. SOA (Service-Oriented Architecture): It is a group of services interacting or communicating with each other. Depending on the nature of the communication, it can be simple data exchange or it could involve several services coordinating some activity.   
3. Microservice Architecture: It involves structuring an application in the form of a cluster of small, autonomous services modeled around a business domain. The functional modules can be deployed independently, are scalable, are aimed at achieving specific business goals, and communicate with each other over standard protocols. 

## 5. how to do load balance in microservice?
Load balancing is an essential component in microservice architectures to distribute incoming requests across multiple instances of a service, ensuring optimal resource utilization and improved performance. There are several approaches to implementing load balancing in microservices:

Client-Side Load Balancing:

The client-side load balancing approach involves implementing load balancing logic within the client application.
The client application uses a load balancer component or library to distribute requests across multiple instances of a service.
Popular libraries for client-side load balancing include Netflix Ribbon and Spring Cloud LoadBalancer in the Java ecosystem.
The client-side load balancer typically retrieves service instances from a service registry and uses algorithms like round-robin, random, or weighted load balancing to route requests.
Server-Side Load Balancing:

In server-side load balancing, a separate load balancer component sits between the client and the microservices.
The load balancer receives incoming requests and decides which service instance should handle the request.
The load balancer uses algorithms like round-robin, least connections, or weighted load balancing to distribute requests.
Popular tools for server-side load balancing include NGINX, HAProxy, and Envoy Proxy.
The load balancer may retrieve service instances from a service registry or use predefined configurations.
Service Mesh Load Balancing:

Service meshes, like Istio or Linkerd, provide advanced load balancing capabilities as part of their feature set.
They introduce a sidecar proxy (e.g., Envoy) alongside each service instance, enabling fine-grained control over request routing and load balancing.
Service meshes typically use dynamic load balancing algorithms and offer features like traffic splitting, canary deployments, and fault tolerance mechanisms.
Cloud Provider Load Balancing:

Cloud providers offer their own load balancing solutions tailored for microservices in their cloud environment.
For example, AWS provides Elastic Load Balancer (ELB), Azure offers Azure Load Balancer, and Google Cloud Platform provides Google Cloud Load Balancing.
These load balancers are usually integrated with the cloud provider's infrastructure and provide scalability, fault tolerance, and traffic management features.

## 6. How to do service discovery?
DNS-Based Service Discovery:
DNS-based service discovery relies on the Domain Name System (DNS) to discover services. In this approach, each service registers itself with a DNS server, providing its hostname and IP address. Clients can then query the DNS server to obtain the IP addresses of the services they need. Some popular implementations of DNS-based service discovery include DNS Service Discovery (DNS-SD) and the Service Discovery Protocol (SDP).

Service Registry and Discovery:
In this approach, a central service registry or directory is used to keep track of available services. Each service registers itself with the registry upon startup, providing information such as its name, location, and any metadata. Clients can then query the registry to discover the available services. Some popular service registry and discovery solutions include Apache ZooKeeper, Consul, and etcd.

Self-Registration:
In this method, services register themselves with each other directly, without relying on a central registry. When a service starts up, it announces its presence to other services by broadcasting a message or sending a request to a well-known location. Other services listening for these announcements can then discover and establish connections with the newly available service. This approach is often used in peer-to-peer architectures or within small-scale systems.

Proximity-Based Discovery:
Proximity-based discovery utilizes the physical or network proximity of services to discover each other. It can be implemented using techniques such as multicast or broadcast messaging, where services advertise their availability to other services within the same network segment. This approach is useful in scenarios where services are located in the same local network or physical proximity is relevant.

## 7. What are the major components of Kafka?
Topics:
Topics in Kafka represent the categories or feeds to which messages are published. They act as logical containers for organizing and categorizing the data streams. Producers write messages to topics, and consumers read messages from topics. Topics are partitioned to allow for parallelism and scalability.

Producers:
Producers are responsible for publishing (writing) messages to Kafka topics. They send messages to a specific topic or a set of topics. Producers can be implemented in various programming languages and frameworks. They can also choose to specify the partition to which each message should be sent or allow Kafka to handle the partitioning automatically.

Consumers:
Consumers read messages from Kafka topics. They subscribe to one or more topics and consume messages published to those topics. Consumers can be part of a consumer group, where each consumer in the group reads from a different subset of the partitions within a topic. This allows for scalability and parallel processing of messages.

Brokers:
Brokers are the Kafka servers that handle the storage, replication, and distribution of the topic partitions. They form the distributed cluster of Kafka. Each broker manages one or more topic partitions, serving as the storage and processing unit for the messages. Brokers handle the replication of data across multiple nodes to ensure fault tolerance and high availability.

Topics and Partitions:
Topics are divided into one or more partitions. Each partition is an ordered and immutable sequence of messages. Partitions allow for parallelism and distributed processing of messages. The number of partitions determines the parallelism and throughput of the system. Kafka stores the messages in partitions and retains them for a configurable period.

Kafka Connect:
Kafka Connect is a framework and set of connectors that simplify the integration of Kafka with external systems. It enables the ingestion and egress of data between Kafka and various data sources or sinks. Connectors are used to define the configuration and behavior of data pipelines, allowing data to flow between Kafka and other systems seamlessly.

Kafka Streams:
Kafka Streams is a client library that enables the development of real-time stream processing applications using Kafka. It provides high-level APIs for stream processing operations, such as filtering, transforming, aggregating, and joining streams of data. Kafka Streams allows developers to build stateful and fault-tolerant stream processing applications that can process and analyze data in real-time.

## 8. What do you mean by a Partition in Kafka?
In Kafka, a partition is a fundamental unit of scalability and parallelism within a topic. Topics in Kafka are divided into multiple partitions, and each partition is an ordered and immutable sequence of messages.

## 9. What do you mean by zookeeper in Kafka and what are its uses?
In Kafka, Apache ZooKeeper is a centralized coordination service that plays a critical role in the reliable and distributed operation of a Kafka cluster. It is used for various purposes, including maintaining cluster metadata, managing leadership election, and facilitating synchronization between Kafka brokers.

## 10. Can we use Kafka without Zookeeper?
No, as of the latest available information (September 2021), Apache Kafka requires Apache ZooKeeper for its core functionality. Kafka relies on ZooKeeper for critical operations such as maintaining cluster metadata, managing leader election, and coordinating partition assignments.

## 11. Explain the concept of Leader and Follower in Kafka.
Leader:

The leader replica is responsible for handling all read and write operations for a specific partition within a topic.
Producers send messages to the leader replica, which then appends them to the end of the partition's log in the order they are received.
The leader coordinates the replication of messages to the follower replicas and ensures that the followers are kept up-to-date with the latest data.
Consumers read messages from the leader replica, maintaining the order of the messages within the partition.
If the leader replica fails or becomes unavailable, a new leader is elected among the available replicas.
Follower:

Follower replicas replicate the data from the leader replica for a partition.
Followers continuously pull data from the leader replica and replicate it by appending it to their own local copies of the partition's log.
Follower replicas maintain synchronization with the leader replica, ensuring that they have the latest data.
Followers do not serve read or write requests directly from clients; their main responsibility is to replicate the data from the leader.
If a follower replica falls behind or becomes disconnected from the leader for an extended period, it may be considered out-of-sync and will need to catch up with the leader before resuming its replication role.

## 12. Why is Topic Replication important in Kafka? What do you mean by ISR in Kafka?
Topic replication is an important feature in Kafka that provides fault tolerance, high availability, and data durability. It involves replicating a topic's partitions across multiple brokers within a Kafka cluster. Each partition has a leader replica and one or more follower replicas.

ISR (In-Sync Replicas), it refers to the subset of replicas that are considered "in sync" with the leader replica. In Kafka, each partition has an ISR, which consists of the replicas that have replicated all the messages up to a certain point (known as the high watermark). The ISR is dynamically maintained by Kafka, and it ensures that replicas in the ISR are up-to-date with the leader replica.

## 13. What do you understand about a consumer group in Kafka?
In Kafka, a consumer group is a logical grouping of consumers that work together to consume and process messages from Kafka topics. It allows for parallel and scalable consumption of messages across multiple consumers within the same group. 

## 14. How do you start a Kafka server?
Navigate to the Kafka installation directory in your terminal or command prompt.

Start the ZooKeeper service:
```
bin/zookeeper-server-start.sh config/zookeeper.properties
```

Start the Kafka server:
```
bin/kafka-server-start.sh config/server.properties
```

## 15. Tell me about some of the real-world usages of Apache Kafka.
Event Streaming and Real-time Data Pipelines:
Kafka's ability to handle high-throughput, low-latency data streams makes it well-suited for building real-time data pipelines and event streaming applications. It is used for collecting, processing, and distributing streaming data in real-time across different systems and components.

Messaging Systems and Decoupled Architectures:
Kafka serves as a highly scalable and fault-tolerant messaging system, enabling reliable communication between different applications and services. It decouples producers and consumers, providing a reliable, asynchronous, and distributed messaging infrastructure.

Log Aggregation and Stream Processing:
Kafka is often used as a central log aggregation system, where log events from various sources are ingested and stored. These logs can be processed in real-time using Kafka Streams or other stream processing frameworks for tasks such as anomaly detection, monitoring, analytics, and more.

## 16. Describe partitioning key in Kafka.
In Kafka, a partitioning key is an attribute or value associated with a message that determines which partition within a topic the message will be written to.

## 17. What is the purpose of partitions in Kafka?
Scalability and Parallelism:
Partitions allow Kafka to scale horizontally and handle large amounts of data and high message throughput. By dividing a topic into multiple partitions, Kafka can distribute the load across multiple brokers and enable parallel processing. Each partition can be assigned to a different broker, allowing for concurrent read and write operations on different partitions, thus increasing overall throughput.

Message Ordering:
Within a partition, Kafka guarantees the order of messages. Messages produced to a specific partition are stored in the order of their arrival. This order is maintained as messages are consumed from the partition, ensuring that consumers process the messages in the same order they were produced. This property is critical for maintaining data integrity and consistency in applications that rely on message ordering.

Data Retention and Durability:
Kafka retains messages in partitions for a configurable period of time. This durability allows consumers to consume historical data by specifying an offset to read from. Partitions enable Kafka to handle large volumes of data over extended periods by distributing the data across multiple brokers, ensuring fault tolerance, and avoiding data loss.

Consumer Group Parallelism:
Consumer groups in Kafka allow multiple consumers to work together to consume messages from a topic. Each consumer within a group is assigned a subset of partitions to read from. Partitions enable parallel consumption as each consumer can independently read messages from their assigned partition(s). This parallelism improves the throughput and processing capabilities of the consumer group.

Fault Tolerance and High Availability:
Partition replication in Kafka provides fault tolerance and high availability. Each partition can have multiple replicas distributed across different brokers. If a broker fails or becomes unavailable, one of the replicas can be elected as the new leader to continue serving the partition. Replication ensures that data is not lost and allows for continuous operation even in the face of failures.

Load Balancing:
Kafka dynamically distributes partitions across brokers to achieve load balancing. When new brokers are added to the cluster or existing brokers are removed, Kafka automatically rebalances the partition assignments. This distribution ensures that the workload is evenly spread across brokers, preventing hotspots and optimizing resource utilization.

## 18. Differentiate between Rabbitmq and Kafka.
Messaging Model:
RabbitMQ follows the traditional message queueing model, where messages are produced by publishers and consumed by subscribers. It supports various messaging patterns like point-to-point, publish/subscribe, and request/reply. Kafka, on the other hand, is based on a distributed commit log architecture that focuses on handling streams of records. It is designed for real-time data streaming, event sourcing, and processing of high-throughput data.

Data Persistence:
RabbitMQ stores messages persistently by default, ensuring that messages are not lost even if the server or broker fails. It offers durability through disk-based storage. In Kafka, messages are stored in a distributed commit log for a configurable period. Kafka provides fault tolerance and durability by replicating messages across multiple brokers.

Scalability and Throughput:
Kafka is known for its high scalability and throughput capabilities. It is designed to handle large volumes of data and is particularly suitable for use cases with high message rates and real-time processing. Kafka achieves scalability by distributing partitions across multiple brokers and allowing parallel processing. RabbitMQ can also handle significant message loads but may have more limitations in terms of scalability compared to Kafka.

Message Ordering and Event Processing:
In RabbitMQ, message ordering is preserved within each queue, ensuring that messages are processed in the order they were received. RabbitMQ provides rich support for features like message acknowledgment, redelivery, and message priority. Kafka, on the other hand, guarantees message ordering within a partition, enabling strong event processing semantics. Kafka is optimized for handling streams of events and supports high-throughput, real-time processing and analytics on data streams.

Integration Patterns:
RabbitMQ provides a wide range of client libraries and supports multiple protocols like AMQP, MQTT, and STOMP, making it suitable for integrating various systems and technologies. It has broad language support and is widely used in enterprise messaging scenarios. Kafka, on the other hand, focuses on real-time data streaming and has a simple binary protocol. It provides client libraries primarily for Java and offers various connectors and integrations for popular data processing frameworks like Apache Spark and Apache Flink.

Use Cases:
RabbitMQ is commonly used for traditional enterprise messaging, task queues, job processing, and general-purpose queuing scenarios. It is suitable for scenarios where message persistence, fine-grained control over messaging patterns, and guaranteed delivery are critical. Kafka is widely used for real-time streaming, event sourcing, log aggregation, clickstream analysis, and building data pipelines. It excels in scenarios that require high scalability, fault tolerance, and real-time processing of large data streams.

## 19. What are the guarantees that Kafka provides?
Message Persistence:
Kafka persists messages to disk, ensuring durability and preventing data loss. Messages written to Kafka topics are stored in partitions, and these partitions are replicated across multiple brokers. This replication mechanism ensures that even if a broker fails, the data is still accessible from the replicas.

Fault Tolerance and High Availability:
Kafka's distributed architecture and replication mechanism provide fault tolerance and high availability. If a broker fails, Kafka automatically promotes one of the replicas as the new leader for the affected partitions. This ensures continuous operation and allows clients to consume and produce messages without disruption.

Order Preservation within Partitions:
Kafka guarantees the order of messages within a partition. Messages published to a specific partition are stored in the order of their arrival and are consumed in the same order. This ensures that the order of events or messages is maintained within a partition, enabling applications that rely on sequential processing or event sourcing.

Exactly-Once Semantics:
Kafka supports "at-least-once" message delivery semantics by default, ensuring that messages are not lost even in the presence of failures. Additionally, Kafka provides mechanisms to achieve "exactly-once" semantics through transactional writes and consumer offsets management. This allows producers and consumers to achieve end-to-end exactly-once processing guarantees when properly configured.

Scalability and High Throughput:
Kafka is designed to handle high message throughput and scale horizontally. By distributing partitions across multiple brokers, Kafka allows for parallel processing and enables high concurrency. It can handle millions of messages per second and scales seamlessly as more brokers are added to the cluster.

Data Retention and Replayability:
Kafka retains messages for a configurable period, allowing consumers to replay or process historical data. Messages are stored on disk, and consumers can specify an offset to read from, allowing them to consume past messages as needed. This feature is particularly useful for building data pipelines, analytics, and real-time stream processing.

## 20. What do you mean by an unbalanced cluster in Kafka? How can you balance it?
An unbalanced cluster in Kafka refers to a situation where the distribution of partitions and workload across brokers is uneven. It means that some brokers may be overloaded with more partitions and message traffic, while others are underutilized.

Automatic Partition Rebalancing:
Kafka provides built-in mechanisms to automatically rebalance partitions across brokers when the cluster topology changes. This includes scenarios such as adding or removing brokers, or when new topics or partitions are created. Kafka's partition rebalancing ensures that partitions are evenly distributed among available brokers, helping to achieve load balancing and scalability.

Adjusting Replication Factors:
By increasing the replication factor for a topic, you can distribute the partitions across more brokers, reducing the load on individual brokers. This approach enhances fault tolerance and allows for better resource utilization. However, keep in mind that increasing the replication factor incurs additional storage and network overhead, so it should be balanced with the available resources.

Manually Reassigning Partitions:
In some cases, manual partition reassignment may be necessary to rebalance the cluster. Kafka provides the kafka-reassign-partitions.sh tool to facilitate this process. With careful planning and analysis of the workload and broker capacities, you can manually assign partitions to distribute the workload evenly across brokers. However, this approach requires caution and should be performed during low traffic periods to avoid disrupting ongoing operations.

Monitoring and Capacity Planning:
Regular monitoring of broker and topic metrics is crucial to identify imbalances in the cluster. Monitoring tools like Apache Kafka's built-in metrics or third-party monitoring solutions can provide insights into broker resource utilization, partition distribution, and message traffic. Based on this information, you can perform capacity planning and take proactive measures to prevent or address imbalances before they become critical.

Scaling the Cluster:
If an unbalanced cluster persists due to high message throughput or resource limitations, scaling the Kafka cluster by adding more brokers can help distribute the workload and achieve better balance. Adding additional brokers increases the available capacity and allows for better resource allocation and partition distribution.

## 21. In your recent project, are you a producer or consumer or both?
Both

## 22. In your recent project, Could you tell me your topic name?
Shopping cart management

## 23. In your recent project, How many brokers do you have? How many partitions for each topic? How many data for each topic.
10 brokers. 10 partitions for each topic. Each topic has about 2500 records.

## 24. In your recent project, which team produce what kind of event to you and you producer what kind of events?
I'm with the "Customer Notifications Team":
Produces events: "OrderConfirmationEmail", "ShippingNotification"
Consumes events: "OrderPlaced", "OrderUpdated"

## 25. What is offset?
In Apache Kafka, an offset is a unique identifier that represents the position of a consumer within a partition of a topic. It denotes the progress of a consumer in consuming messages from a specific partition. 
### 1. list all of the new annotations you learned to your annotations.md

### 2. Document the microservice architeture and components/tools/dependencies
1. API Gateway: routes requests to corrsponding microservices. It is also used to handle filter or security concerns (Zuul)

2. Load Balancer: distributes incoming requests across multiple instances microservice to ensure high availability (Ribbon)

3. Circuit Breaker: prevents and manages cascading failures between services. Hystrix is a latency and fault tolerance library designed to isolate points of access to remote systems, services, and 3rd party libraries, and stop failures. (Hystrix)

4. Service Registry (discovery): enables service discovery and communication between microservices. It is a centralized repository where microservices can register themselves and provide information about their location, availability, and endpoints. (Eureka, Nacos)

5. Configuration: is used to centralize and manage configuration settings. (Consul)

6. Message Queue: enables asynchronous communication and decoupling of microservices. Apache Kafka is a distributed event streaming platform capable of handling trillions of events a day. (Kafka, RabbitMQ)

### 3. What are Resilience patterns? What is circuit breaker?
Resilience patterns are design principles used to build robust and reliable systems that can handle failures and disruptions. Such as:

1. Retry: automatically retry a failed operation with the hope that it will succeed on subsequent attempts. It allows for transient failures to be overcome without manual intervention.
2. Bulkhead: divide the system into isolated compartments (or "bulkheads") to limit the impact of failures. If one component fails, it does not bring down the entire system but only affects a specific portion.
3. Timeout: set a maximum time for an operation to complete. If the operation exceeds the defined time limit, it is considered failed or abandoned. Timeouts prevent requests from waiting indefinitely and help maintain system responsiveness.
4. Fallback: define an alternative response or behavior when an operation fails. It allows the system to gracefully handle failures by providing a backup plan or default behavior.
5. Circuit Breaker: is used to prevent a system from continuously executing a failing operation. It monitors the availability of a service and, if the failure rate exceeds a certain threshold, the circuit breaker trips, blocking subsequent requests and providing fallback responses.

### 4. Read this article, then list the important questions, then write your answers
    a. https://www.interviewbit.com/microservices-interview-questions/#main-features-of-microservices

### 5. how to do load balance in microservice? Write a long Summary by yourself.
    a. https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/
    b. https://www.fullstack.cafe/blog/load-balancing-interview-questions
Load balancer do continuous health checks to monitor the servers' capability of handling the request. It minimizes server response time and maximizes throughput. By sending requests only to online servers, load balancer ensures high availability and reliability. Types of load balancers:

1. Load Balancers in Clients: The client application will be provided with a list of web servers to interact with. The application chooses the first one in the list and requests data from the server. If any failure occurs persistently (after a configurable number of retries) and the server becomes unavailable, it discards that server and chooses the other one from the list to continue the process. This is one of the cheapest ways to implement load balancing.

2. Load Balancers in Services: These load balancers receive and redirect requests according to a set of rules.

3. Hardware Load Balancers Device: HLD is a physical appliance used to distribute web traffic across multiple network servers. It keeps doing the health checks on each server and ensure that each server is responding properly. If any of the servers don't produce the desired response, it immediately stops sending the traffic to the servers. These load balancers are expensive to acquire and configure, which is the reason a lot of service providers use them only as the first entry point for user requests.

### 6. How to do service discovery?
Service discovery is a mechanism that allows services in a distributed architecture to find and communicate with each other. It is typically implemented using either a client-side or a server-side discovery pattern. 
Server-Side Service Discovery: In this pattern, a server takes care of the service discovery. Client services make requests via a service discovery router or API gateway. The router queries a service registry and forwards the request to an available service instance. When a service starts, it registers itself with the service registry. The client makes a request via a service discovery router. The router queries the service registry. The router uses a load-balancing algorithm to select one of the available service instances. The router forwards the client's request to the selected service instance.

### 7. What are the major components of Kafka?
1. Producers: the applications or services that publish events/write messages into Kafka topics.

2. Consumers: the applications or services that subscribe to topics and process the stream of records.

3. Topics: A topic is a category or feed name to which messages are published by producers and from which consumers consume messages. Topics are divided into partitions for parallel processing and can be replicated across multiple brokers for fault tolerance.

4. Partitions: Topics can be divided into partitions. Each partition is an ordered, immutable sequence of records that is continually appended to. The records in the partitions are each assigned a sequential id number called the offset.

5. Brokers: Kafka cluster consists of one or more servers. Brokers are responsible for the storage, retrieval, and replication of Kafka data.

6. Replicas: Replication in Kafka provides failover. If a topic has been configured with replication factor N then Kafka will maintain N copies of that topic. The replicas of a partition are distributed over the Kafka brokers.

7. ZooKeeper: Kafka uses ZooKeeper to manage service discovery for Kafka Brokers that form the cluster. ZooKeeper sends changes of the topology to Kafka, so each node in the cluster knows when a new broker joined, a Broker died, a topic was removed or a topic was added, etc.
ZooKeeper: Although not strictly a part of Kafka, ZooKeeper is a separate component often used alongside Kafka. ZooKeeper is a distributed coordination service that helps manage and maintain configuration, synchronization, and leader election for Kafka brokers.

8. Offset: An offset is a unique identifier assigned to each message within a partition. It represents the position of a consumer in the partition's message stream and enables precise control over the consumed messages.

9. Consumer Group: A consumer group is a group of consumer instances that work together to consume messages from a topic. Each consumer in a group is assigned one or more partitions, ensuring that each partition is consumed by only one consumer at a time. Consumer groups enable parallel processing and scale the consumption of messages.

### 8. What do you mean by a Partition in Kafka?
Ordering: Within each partition, records are ordered and each record is assigned a unique offset. Kafka guarantees the order of messages only within a particular partition, not across different partitions in a topic.
Immutability: Once a record is written to a partition, it can't be changed. It remains there for a configurable amount of time, or until some storage limit is reached.
Distribution: Partitions allow for the distribution of data. Each partition can be placed on a separate machine to allow for multiple consumers to read from a topic in parallel.
Parallelism: Producers write data to topics. Topics are split into partitions for parallelism and distributed among the Kafka cluster nodes. Consumers can read from many partitions at once, and increase the consumption throughput by adding more consumer instances to a consumer group.
Fault Tolerance: Partitions are replicated across multiple brokers to ensure fault tolerance. One broker serves as the "leader" for reading and writing data to the partition, while the others are "followers" which passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader.
Offset: Each partitioned message has a unique sequence id known as offset. Offsets are used by Kafka to maintain the current position of a consumer.

### 9. What do you mean by zookeeper in Kafka and what are its uses?
Apache ZooKeeper is a distributed, open-source coordination service for distributed applications. It's often used for maintaining configuration information, naming services, distributed synchronization, and providing group services.

In the context of Apache Kafka, ZooKeeper is used for managing and coordinating Kafka brokers. Here are some of the main uses of ZooKeeper in Kafka:

Broker Registration: Each Kafka broker that starts up, registers itself with ZooKeeper. It shares its IP address and port information so that clients and other brokers know where it resides on the network.
Broker Discovery: When a Kafka client wants to connect to Kafka, it first connects to ZooKeeper to retrieve the list of brokers and then directly connects to the broker. ZooKeeper helps the client to find the brokers.
Topic Configuration Management: Information about the Kafka topics, including the number of partitions for each topic, the location of all the replicas, the configuration of each topic (such as the maximum size of the messages allowed), and which broker is the preferred leader for each partition, etc., are all stored in ZooKeeper.
Leader Election: Kafka uses ZooKeeper to elect a broker as the controller, which is responsible for managing the states of partitions and replicas and performing administrative tasks like reassigning partitions. It also detects broker failures for leader election.

### 10. Can we use Kafka without Zookeeper?
Historically, Kafka has depended on ZooKeeper for a variety of tasks, including broker registration and discovery, leader election, topic configuration, and maintaining the state of the Kafka cluster.

However, since Apache Kafka version 2.8.0, which was released in 2021, Kafka has started the process of removing this dependency. This mode is called KRaft (Kafka Raft Metadata mode), and it allows Kafka to operate without ZooKeeper. It means Kafka's metadata handling is fully self-contained, reducing operational complexity and improving various aspects of reliability and scalability.

### 11. Explain the concept of Leader and Follower in Kafka.
In Apache Kafka, the concepts of leader and follower pertain to replication, which is a key mechanism for providing the system with fault tolerance and high availability. When a topic is created with replication factor N, Kafka creates N identical replicas (copies) of each partition of that topic. One of these replicas will be designated as the leader, and the rest as followers.

Leader: The leader replica handles all reads and writes for the corresponding partition. When producers want to send data ( messages), they send it to the leader partition. Similarly, when consumers want to read data, they read it from the leader. The leader is responsible for updating its partition with new data and keeping it available to consumers.

Follower: Followers are passive replicas of the leader. Their primary job is to replicate the leader partition in a sync or async manner. They constantly pull data from the leader to stay as up-to-date as possible. Followers don't serve client requests; they exist to provide redundancy and fault tolerance.


### 12. Why is Topic Replication important in Kafka? What do you mean by ISR in Kafka?
In Kafka, topic replication is critical for achieving fault tolerance. When you create a topic in Kafka, you can specify a replication factor, which is the number of copies of each partition of the topic that Kafka will create. The copies are distributed across the brokers in the Kafka cluster.

The importance of replication comes into play when a broker in the Kafka cluster fails. If a broker goes down and it is the leader for some partitions, those partitions won't be available anymore unless you have replicas on other brokers. By having multiple replicas of each partition, Kafka ensures that your data is still available in the face of broker failures.

Furthermore, replicas also enable Kafka to serve more consumers than a single broker could handle. While only the leader replica can serve write requests, both the leader and the follower replicas can serve read requests.

In-Sync Replicas (ISR) in Kafka:

In-Sync Replicas (ISR) is a concept in Kafka pertaining to the current set of "alive" replicas for a particular partition that are fully caught up with the leader.

At any given time, only a subset of the replicas is usually in sync, meaning they have copied all of the messages from the leader. This set is what is referred to as the ISR.

Kafka always chooses the new leader of a partition from the ISR. This ensures that any written message was acknowledged by the new leader before it was elected, which in turn guarantees that written messages are not lost as long as at least one replica remains in sync.

The list of ISRs is maintained by the leader, and this information is periodically sent to ZooKeeper (or directly to the controller in the newer versions of Kafka).

If a follower replica fails to stay caught up with the leader or if it goes offline, it will be removed from the ISR. It will only be added back to the ISR if it catches up with the leader again.

In conclusion, topic replication and ISR are key features that provide Kafka with its fault-tolerance and durability guarantees.

### 13. What do you understand about a consumer group in Kafka?
In Kafka, a Consumer Group is a concept that allows a pool of consumers to divide up the work of processing records from one or more topics.

When multiple consumers are subscribed to a topic and belong to the same consumer group, each consumer in the group will receive different subsets of the records from the topic. Essentially, the group of consumers will jointly consume the data from the topic, with each consumer in the group consuming a unique set of partitions.

Load Balancing: Each consumer in a group will read messages from a unique subset of partitions of the topics it subscribes to, so the group as a whole will effectively load balance the consumption of records.
Fault Tolerance: If a consumer fails (goes offline or crashes), the load will be automatically distributed to the remaining consumers in the group, providing built-in fault tolerance.
Parallel Processing: Consumer groups allow for the records of a topic to be consumed in parallel, which is a key requirement in high-volume, low-latency processing pipelines.
Offset Management: Each consumer group maintains its offset per topic partition. The offset is the position till where the consumer group has consumed the messages in each partition.
Single Consumer Multiple Groups: If every consumer is in its own consumer group, then each consumer will receive all the records from the topics it's subscribed to, as if it was a traditional message queue rather than a topic.
Multiple Consumers Single Group: If all consumers are part of the same consumer group, the records from the topic(s) will be effectively load balanced over the consumers, as if it was a queue that supported multiple consumers.

### 14. How do you start a Kafka server?
Start Zookeeper Server: Kafka uses ZooKeeper, so you need to first start a ZooKeeper server if you haven't already. You can use the convenience script packaged with Kafka to get a quick-and-dirty single-node ZooKeeper instance.

From the Kafka directory, run the following command: bin/zookeeper-server-start.sh config/zookeeper.properties

Start Kafka Server: Once ZooKeeper is up and running, you can then start the Kafka server. From the Kafka directory, run the following command: bin/kafka-server-start.sh config/server.properties

### 15. Tell me about some of the real-world usages of Apache Kafka.
Real-time Streaming and Processing: Kafka is often used in real-time data streaming and processing platforms. These systems continually ingest and process high volumes of data in real time. An example of this could be an IoT system where devices continually send data about their state to a Kafka topic, and this data is processed as soon as it arrives.
Messaging: Kafka can be used as a replacement for a traditional messaging system. It provides functionality similar to a messaging system, with the added benefits of fault tolerance, scalability, and durability.
Log Aggregation: Kafka is used as a central system for collecting logs from multiple services, which are then consumed by multiple consumers, like log analysis tools or monitoring services.
Event Sourcing: Event sourcing is a style of application design where state changes are logged as a time-ordered sequence of records. Kafkaâ€™s support for storing a large amount of data makes it an excellent backend for an event sourcing system.
Metrics: Kafka can be used to record metrics from distributed applications. These metrics can then be consumed and aggregated for real-time monitoring.
Activity Tracking: Kafka is often used to track activities in web and mobile applications. For instance, it can be used to record page views, clicks, or other user activity.
Microservices Architecture: Kafka can act as the communication and integration point between various microservices. It is particularly helpful when you need to implement the asynchronous communication between the services.

### 16. Describe partitioning key in Kafka.
In Kafka, the partition key is used to determine the specific partition where a message will be written. When a producer sends a message to a Kafka topic, one of the parameters it can set is the key. The key is a piece of metadata that accompanies the message.

The key plays an essential role when it comes to distributing messages across different partitions of a Kafka topic. By default, Kafka uses a round-robin strategy to send messages to different partitions if a key is not specified. However, if a key is provided, Kafka uses a hash of the key to determine the partition. This ensures that all messages with the same key will always go to the same partition (assuming the number of partitions doesn't change).

Why is this useful? By ensuring that all messages with the same key (for example, a user ID or some other unique identifier) land in the same partition, Kafka guarantees that those messages will be read in the order they were written. This is because Kafka only guarantees a total order of messages within a single partition, not across different partitions in a topic.

It's important to note that while keys can be used to control the distribution of messages, if keys are not uniformly distributed this could create an imbalance of data across partitions. So care should be taken when designing the key strategy to ensure a balanced load across partitions.

### 17. What is the purpose of partitions in Kafka?
Scalability: Partitions are a way of scaling a Kafka topic across many servers. A Kafka topic can be divided into multiple partitions, and these partitions can be hosted on different Kafka brokers in a cluster. This allows a topic to handle an amount of data that can't fit on a single machine.
Parallelism: Multiple consumers can read from different partitions concurrently, providing high data processing throughput. In a consumer group, each consumer can read from a unique subset of partitions of the topic, which effectively balances the consumption of records.
Fault Tolerance and High Availability: With replication, each partition can be replicated across multiple brokers. If one broker fails, another broker that hosts the replica of the partition can take over.
Ordering Guarantees: Kafka guarantees the order of messages within a single partition, not across partitions in a topic. This means that if order is important in your use case, you can use a key to ensure that all related messages are written to the same partition in order.
Increased Producer Throughput: Producers can write to multiple partitions simultaneously, which can significantly increase the write throughput.

### 18. Differentiate between Rabbitmq and Kafka.
When to Use RabbitMQ:

Complex Routing Needs: If your system requires a complex routing system, such as sending messages based on multiple criteria or conditions, RabbitMQ could be a good choice because it supports multiple exchange types like direct, topic, headers and fanout.

Request/Reply Patterns: RabbitMQ's support for synchronous request/reply patterns can be an advantage in applications where a sender expects a response to a message.

Delayed Messaging: If you need to delay the delivery of messages, RabbitMQ has built-in support for this.

Single Message Queues: If your use case is primarily centered around processing individual messages (as opposed to streams of messages), RabbitMQ is often a good fit.

Ease of Use: If you need a message broker that is easy to set up and comes with a built-in user-friendly management interface, RabbitMQ might be the better choice.

When to Use Kafka:

Event Sourcing and Logging: Kafka is a good fit if you're dealing with event sourcing, where every change to the state of an application needs to be logged and ordered.

Stream Processing: If your application needs to consume, process, and produce real-time streams of data, Kafka, with its Stream API and KSQL, is a powerful choice.

Massive Throughput: Kafka is designed for high volume, high throughput, and low latency transmission of records. If your use case involves ingesting, processing, and moving high volumes of data with low latency, Kafka is well-suited.

Durability: If your system requires strong durability guarantees for messages, Kafka's persistent commit log architecture is beneficial.

Reprocessing Capability: Kafka retains all messages for a set amount of time (or even indefinitely), regardless of whether they have been consumed. This feature allows you to reprocess events in case of failures or changes in business logic.

### 19. What are the guarantees that Kafka provides?
Message Durability: Messages sent to Kafka are persisted on disk and replicated within the cluster to prevent data loss. The level of durability can be configured based on the acknowledgement settings and the number of replicas.

High Throughput: Kafka is designed to handle high volume real-time data streams, and it can support high throughput for both publishing and subscribing.

Fault Tolerance: Kafka is highly available and resilient to node failures and supports automatic recovery. With replication, if one broker fails, the replicas on other brokers ensure that no data is lost and that service can continue without interruption.

Ordering Guarantees: Within a partition, Kafka guarantees the order of messages. That is, if message M1 is sent before message M2 by the same producer, and M1 and M2 are sent to the same partition, then M1 will be stored before M2 and will have a lower offset.

At-Least-Once Delivery: Kafka ensures that messages sent from a producer to a topic partition will be stored at least once. And when a consumer reads messages, it can also ensure at least once processing. Exactly-once semantics can also be achieved with idempotent producers and transactions, but this often comes with additional configuration and processing overhead.

Scalability: Kafka is designed to be horizontally scalable. That is, it expands its capabilities (whether it's more storage or higher throughput) by simply adding more machines to the Kafka cluster.

### 20. What do you mean by an unbalanced cluster in Kafka? How can you balance it?
In Kafka, an unbalanced cluster usually refers to a state where the distribution of partitions across the brokers in the cluster is uneven. This could occur due to a number of reasons like adding new brokers, brokers going down or coming back up, etc.

Balancing a Kafka cluster usually involves reassigning the partitions across the brokers. This can be done manually using the Kafka partition reassignment tool, but it's a complex task and needs to be done carefully to avoid data loss.

### 21. In your recent project, are you a producer or consumer or both?
Both producer and consumer, it could produce events such as "Order Placed", "Product Viewed", and consume events like " Inventory Updated", "Payment Processed"

### 22. In your recent project, Could you tell me your topic name?
Topics names: "Orders", "ProductViews", "UserActivity", "InventoryUpdates"

### 23. In your recent project, How many brokers do you have? How many partitions for each topic? How many data for each topic.
20 Brokers
800 partitions per topic
Millions messages per day.

### 24. In your recent project, which team produce what kind of event to you and you producer what kind of events?
The Orders team might produce "Order Placed" events to the "Orders" topic whenever a user places an order. The Inventory team could consume these "Order Placed" events and update the inventory accordingly.

The Inventory team might produce "Inventory Updated" events to the "InventoryUpdates" topic whenever the inventory changes. The Product team could consume these "Inventory Updated" events to reflect the changes on the product page.

### 25. What is offset?
In Apache Kafka, an offset refers to the unique identifier of a record within a partition. It denotes the position of a record within the partition.

Kafka maintains a numerical offset for each record in a partition. This offset acts as a kind of pointer and allows Kafka to maintain the order of messages, as messages within a partition are stored and consumed in the order of their offsets.

When a new message arrives at a Kafka partition, it is appended to the end of the partition, and its offset is one larger than the offset of the previous message.

Consumers use this offset to keep track of the messages they have read. When a consumer reads a message from a partition, it will also read the offset. The consumer can store this offset and use it in the future to resume consumption from the point where it left off. If a consumer has processed up to offset 10, for example, it will begin with offset 11 when it resumes reading.

Kafka retains messages in partitions for a configurable amount of time, irrespective of whether messages have been consumed or not. So, if a consumer has been offline or not consuming messages for a while, it can resume consuming from the offset it last read, provided the messages are still within the configured retention period.

This feature of Kafka allows for high-throughput and low-latency consumption of real-time data with fault tolerance.
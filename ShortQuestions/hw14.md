# 1. list all of the new annotations you learned to your annotations.md
# 2. Document the microservice architeture and components/tools/dependencies
First, the request is sent externally, and then passed to the containerized service through API Gateway. Connect to the database outside the service for data storage. Ribbon can perform client load balancing. Hystrix provides fault-tolerant resilience in distributed systems. He can isolate the failed service and house the failure. Microservices can publish events to Kafka topics and subscribe to related topics to consume events. Microservices register themselves with the Eureka service registry at startup, enabling dynamic service discovery.
- `Spring Cloud` A tool provides to the microservice.
- `Zuul API Gateway` As the entry point for external clients, utilizes Ribbon for client-side load balancing.
- `Eureka` Each microservice registers itself with the Eureka service registry upon startup. Eureka maintains a registry of available services and their network locations. Microservices use Eureka to discover and communicate with other services dynamically, without hardcoded dependencies.
- `Ribbon Load Balancer` Ribbon distributes incoming requests across multiple instances of a microservice. It ensures even distribution of traffic and directs requests to healthy instances, enhancing scalability and fault tolerance.
- `Hystrix Circuit Breaker` Hystrix can isolate failing services and prevent cascading failures. It provides fallback mechanisms and implements strategies like request caching and request collapsing.
- `Config Server` provides centralized configuration management for microservices.
- `Kafka` Microservices communicate asynchronously using Kafka as the event bus. Microservices can publish events to Kafka topics and subscribe to relevant topics to consume events. Kafka enables loose coupling and scalability by decoupling services through event-based communication.
- `Docker` Microservices are containerized using Docker, which provides a lightweight, isolated environment for each service, ensuring portability and consistency.
# 3. What are Resilience patterns? What is circuit breaker?
Resilience patterns are design patterns and techniques used to build resilient and fault-tolerant systems that can gracefully handle failures and disruptions in a distributed environment. These patterns aim to improve the overall stability and reliability of a system by mitigating the impact of failures and providing mechanisms to recover from them.

The Circuit Breaker pattern is used to prevent cascading failures and to provide fault tolerance when invoking remote services or accessing resources that may experience temporary failures or become unresponsive. It acts as a safety mechanism that helps to control and manage the flow of requests.
# 4. Read this article, then list the important questions, then write your answers
- `Write main features of Microservices.`
Decoupling, Componentization, Business Capabilities, Continous Delivery, Responsibility, Deventralized Governance, Agility.
- `Write main components of Microservices.`
1. Containers, Clustering, and Orchestration 

2. IaC [Infrastructure as Code Conception] 

3. Cloud Infrastructure 

4. API Gateway 

5. Enterprise Service Bus 

6. Service Delivery
- `Explain spring cloud and spring boot.`
Spring Boot simplifies the development of standalone Spring applications, while Spring Cloud extends it further by providing additional tools and frameworks specifically designed for building and managing distributed systems and microservices architectures. Spring Boot provides the foundation, and Spring Cloud adds the necessary features and abstractions to create scalable, resilient, and cloud-native microservices applications.
- `What do you mean by Cohesion and Coupling?`
`Cohesion` refers to the degree to which the responsibilities and functionality within a single module or component are related and focused.
High cohesion indicates that the elements within a module are strongly related and contribute to a single well-defined responsibility or functionality.

`Coupling` refers to the degree of interdependence and interaction between modules or components within a system. Coupling indicates the strength of the relationships and dependencies between different modules.
- `Explain the term Eureka in Microservices.`
Eureka acts as a service registry or directory where microservices can register themselves upon startup. It allows services to be dynamically discovered and located by other services without the need for explicit configuration or hardcoding of service endpoints.
- `What is the main role of docker in microservices?`
The main role of Docker in microservices is to provide containerization technology that allows for the packaging, distribution, and deployment of microservices as lightweight and isolated containers.
# 5. how to do load balance in microservice? Write a long Summary by yourself.
It simply distributes the set of requested operations (database write requests, cache queries) effectively across multiple servers and ensures that no single server bears too many requests that lead to degrading the overall performance of the application.
# 6. How to do service discovery?
Service discovery is the process of dynamically locating and accessing services in a distributed system. It allows components or services to discover and communicate with each other without hardcoded dependencies.
# 7. What are the major components of Kafka?
- `broker` A broker is a Kafka server that manages the storage and replication of message logs. It serves as the central component responsible for handling read and write requests from producers and consumers. 
- `Topic` A topic is a category or feed name to which producers publish messages, and from which consumers consume messages.
- `Producer` Producers are applications or systems that publish messages to Kafka topics. They write messages to specific topics, and Kafka takes care of storing and replicating them across multiple brokers.
- `Comsumer` Consumers are applications or systems that read messages from Kafka topics. They subscribe to one or more topics and consume messages from the partitions assigned to them.
- `Partition` A partition is a unit of parallelism and scalability in Kafka. Topics are divided into multiple partitions, and each partition is ordered and immutable.
- `Offset` Offsets are used to track the progress of a consumer within a partition.
- `Consumer Group`  A consumer group is a group of consumers that work together to consume messages from one or more topics.
- `Kafka Connect` Kafka Connect is a framework that provides an integration mechanism for connecting Kafka with external systems. It allows you to easily import data from external sources or export data from Kafka to external sinks.
- `Kafka Streams` Kafka Streams is a client library for building real-time streaming applications and microservices. It allows developers to process and analyze data directly within Kafka, without the need for external processing frameworks.
# 8. What do you mean by a Partition in Kafka?
Partitioning is a crucial mechanism in Kafka that enables high-throughput, fault-tolerant, and scalable message processing. It allows Kafka to handle large amounts of data while maintaining ordering guarantees and facilitating parallel consumption and production of messages.
# 9. What do you mean by zookeeper in Kafka and what are its uses?
ZooKeeper is a centralized open-source service that is used for maintaining configuration information, providing distributed coordination, and managing cluster metadata.

Uses:
- ZooKeeper is responsible for coordinating the activities and communication among the various components of a Kafka cluster.
- ZooKeeper is used to elect a leader for each partition of a Kafka topic. Within a partition, one broker is designated as the leader, responsible for handling all read and write requests for that partition.
- ZooKeeper keeps track of the registered brokers and monitors their health. If a broker goes offline or becomes unreachable, ZooKeeper can notify other components of the cluster about the changes in the broker's status.
- ZooKeeper is used to store and manage metadata related to Kafka topics and their partitions. It maintains information about the number of partitions in a topic, the leader and replica assignments for each partition, and the configuration parameters for topics.
- ZooKeeper helps in coordinating consumer groups in Kafka. It stores and tracks the offsets of consumer group members, allowing them to resume consuming from the correct position after failures or restarts. 
- Kafka uses ZooKeeper to store dynamic configuration settings, such as broker settings and topic configurations.
# 10. Can we use Kafka without Zookeeper?
Yes, after Apache Kafka 2.8.0. We can use Kafka without Zookeeper.
# 11. Explain the concept of Leader and Follower in Kafka.
- `Leader` The leader is the primary broker responsible for handling read and write requests for a particular partition of a Kafka topic. The leader is responsible for maintaining the order of messages within its partition. It appends incoming messages to the end of the log, assigning each message a unique offset.
- `Follower` Followers are brokers that replicate the leader's data to ensure fault tolerance and high availability. Followers maintain a copy of the leader's log by continuously pulling replica logs from the leader.
The follower keeps track of the last replicated offset to ensure it stays in sync with the leader.
# 12. Why is Topic Replication important in Kafka? What do you mean by ISR in Kafka?
Replication involves maintaining multiple copies of each partition across different brokers in the cluster. It is crucial for ensuring fault tolerance, data durability, and high availability within a Kafka cluster.

ISR(In-Sync Replicas) refers to the subset of replicas that are synchronized with the leader and actively participate in the replication process. When a leader broker receives a write request, it waits for the replicas in the ISR to acknowledge the successful replication of the message before considering it committed.
# 13. What do you understand about a consumer group in Kafka?
A consumer group is a logical grouping of consumer instances that work together to consume messages from one or more topics. Consumer groups in Kafka enable scalable and fault-tolerant message processing. They allow multiple consumers to work together, sharing the workload and providing redundancy. Consumer groups are commonly used in scenarios where high-throughput, real-time data processing is required, such as stream processing, event-driven architectures, and microservices.
# 14. How do you start a Kafka server?
- Get Kafka
- Start the Kafka environment
- Create a topic to store your events.
- Write some events into the topic-producer
- Read the events- consumer
- Import/Export your data as streams of events with kafka connect.
- Process your events with Kafka streams
- Terminate the Kafka environment.
# 15. Tell me about some of the real-world usages of Apache Kafka.
- Kafka is a popular choice for building real-time data pipelines and event streaming platforms. It can handle high-throughput, low-latency ingestion and delivery of large volumes of data.
- Kafka's log-based architecture makes it an ideal solution for log aggregation, collection, and processing. It can centralize logs from multiple sources, allowing efficient storage, analysis, and monitoring.
- Kafka's publish-subscribe messaging model makes it well-suited for building scalable message queues and pub/sub systems. It provides durable storage and reliable delivery of messages, allowing decoupling of producers and consumers.
- Kafka can serve as a scalable and fault-tolerant data transport layer in data integration and ETL (Extract, Transform, Load) pipelines. It allows streaming data from various sources, such as databases, messaging systems, and log files, to be efficiently collected, transformed, and loaded into data stores or analytical systems.
# 16. Describe partitioning key in Kafka.
A partitioning key is an attribute or value associated with a message that determines the target partition to which the message will be assigned.
# 17. What is the purpose of partitions in Kafka?
By leveraging partitions, Kafka achieves scalability, fault tolerance, ordering guarantees, and parallel processing capabilities. Partitioning is a fundamental concept that allows Kafka to handle large-scale data streams and support diverse use cases across various industries.
# 18. Differentiate between Rabbitmq and Kafka.
RabbitMQ is a mature messaging system that focuses on reliable message delivery, while Kafka is designed for high-throughput, fault-tolerant data streaming and processing.
# 19. What are the guarantees that Kafka provides?
- Kafka provides durability by persisting messages to disk.
- Kafka achieves fault tolerance through data replication.
- Kafka can scale horizontally by distributing topics across multiple brokers and partitions. Each partition can be processed independently, allowing for parallelism and improved throughput.
- Kafka guarantees ordered message processing within a partition.
- Kafka provides at-least-once message delivery semantics. This means that messages are guaranteed to be delivered to consumers, but duplicates may occur.
- Kafka allows us to configure retention policies for topics, defining how long messages should be retained in a topic or based on their size. This enables data lifecycle management and the ability to replay or process historical data.
# 20. What do you mean by an unbalanced cluster in Kafka? How can you balance it?
An unbalanced cluster in Kafka refers to a situation where the distribution of partitions among brokers is uneven. It means that some brokers in the cluster may have a significantly higher or lower number of partitions compared to other brokers. This can lead to an imbalance in workload, performance issues, and suboptimal resource utilization.
# 21. In your recent project, are you a producer or consumer or both?
# 22. In your recent project, Could you tell me your topic name?
# 23. In your recent project, How many brokers do you have? How many partitions for each topic? How many data for each topic.
# 24. In your recent project, which team produce what kind of event to you and you producer what kind of events?
# 25. What is offset?
In Kafka, an offset is a unique identifier that represents the position of a consumer within a partition. It is essentially a sequential number assigned to each message within a partition, starting from zero for the first message. The offset provides a way to track the progress and state of consumption for a consumer group in a topic.
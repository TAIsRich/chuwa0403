# HW14

Claudia Feng



1. list all of the new annotations you learned to your annotations.md

   

2. Document the microservice architeture and components/tools/dependencies

   Spring Cloud, Zulu API Gateway, Eureka - Service Registry, Ribbon - Load Balancer, Hystrix/Resilience4j - Circuit Breaker, Config Server, Kafka - Message Queue, Docker

   

3. What are Resilience patterns? What is circuit breaker?

   Common resilience patterns for microservices include:

   1. **Retries**: If a service fails, the client will attempt to call it again. Retries should be used judiciously, as too many could lead to resource exhaustion.
   2. **Circuit Breaker**: This pattern protects a system from making continual failed requests to a particular service. After a set number of failures, it 'opens' the circuit breaker to prevent further requests. After a 'cooldown' period, it will allow a few requests to pass to see if the service has recovered. If the service is still failing, the circuit breaker will open again.
   3. **Fallbacks**: If a service fails, a fallback can be used to provide a default behavior. This could be returning a default value, showing a cached value, or redirecting to another service.
   4. **Rate Limiting**: This controls the number of requests a client can make to a service within a particular time window. This can protect a service from being overwhelmed by too many requests at once.
   5. **Bulkhead**: This pattern isolates elements within an application so that if one fails, it does not bring down the whole system. For example, resources might be divided so that a single user's heavy use does not impact other users.
   6. **Timeouts**: These limit how long a client will wait for a response from a service. This can stop a client from hanging indefinitely when a service is not responding.

   In a microservices architecture, when a network call to a service fails a certain number of times, the Circuit Breaker stops all calls to that service for a pre-defined period. This gives the failing service time to recover and prevents the failure from cascading to other services. After the timeout period, the Circuit Breaker allows a limited number of test requests to pass through. If those requests succeed, the Circuit Breaker closes and the flow of calls resumes normally. If those requests fail, the Circuit Breaker opens again until another timeout period has passed.

   

4. Read this article, then list the important questions, then write your answers

   a. https://www.interviewbit.com/microservices-interview-questions/#main-features-of-microservices

   

5. how to do load balance in microservice? Write a long Summary by yourself.
    a. https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/
    b. https://www.fullstack.cafe/blog/load-balancing-interview-questions

   

6. How to do service discovery?

   **Client-side discovery pattern**:

   In the client-side pattern, the client service is responsible for finding the network locations of its service dependencies.

   1. When the client service starts, it makes a request to the service registry, which is a database of services, their instances and network locations.
   2. The client receives the locations and caches them.
   3. When the client needs to call another service, it uses its cached locations to make the request.

   This approach is direct, as the client communicates with the services directly, but it requires the client to handle the load balancing since multiple instances of the service can be returned.

   **Server-side discovery pattern**:

   In the server-side pattern, the client services make a request via a service discovery server (router or load balancer).

   1. When a new instance of a service is brought up, it registers itself with the service registry.
   2. When a client needs to call another service, it makes a request to the service discovery server.
   3. The server queries the service registry and routes each request to an available service instance.

   This pattern offloads the work of service discovery to the server, simplifying the client service. However, it can be a single point of failure if not managed properly.

   

7. What are the major components of Kafka?

   1. **Producer**: Producers are the source of data in Kafka. They publish data to the topics of their choice.

   2. **Consumer**: Consumers read data from Kafka topics. In Kafka, consumers are typically part of a consumer group for scalability and fault tolerance.

   3. **Topic**: A topic is a category to which the records are published. Topics in Kafka are always multi-subscriber, which means that a topic can have zero, one, or many consumers that subscribe to the data written to it.

   4. **Broker**: Kafka brokers form the core of the system. They are servers where the published messages are stored. Each broker can handle terabytes of messages without performance impact. Kafka brokers are designed to operate as part of a cluster.

   5. **Zookeeper**: Zookeeper is used for managing and coordinating Kafka brokers. It's responsible for configuration management, leader detection, and coordination between different nodes in a Kafka cluster. Zookeeper service is used to notify producer and consumer about the presence of any new broker in the Kafka system or failure of any broker, or topic partition occurrence.

   6. **Partition**: Topics can be split into partitions for better organization, scalability, and performance. Each message within a partition gets an incremental id, called offset.

   7. **Replica**: Replicas are a fail-safe mechanism. Each partition is replicated across multiple brokers. If a broker fails, one of the replicas takes over the leadership and starts serving client requests.

   8. **Leader**: For each partition, one of the brokers is chosen as the leader. All reads and writes go to the leader and the leader coordinates updating replicas with new data.

   9. **Consumer Groups**: This is a group of consumers which jointly consume data from one or more Kafka topics. The group assures that each partition is only consumed by one consumer in the group, and ensures load balance among all the consumers in the group.

   10. **Controller**: Controller is one of the brokers in the Kafka cluster which is responsible for maintaining the leader/follower relationship for all the partitions. When a node shuts down, it's the controller that tells other replicas to become partition leaders to replace the partition leaders on the node that is going away.

   11. **Offsets**: In Kafka, offset is a pointer to the record in a Kafka topic. It is used to maintain the current position of the consumer.

       

8. What do you mean by a Partition in Kafka?

   Topics can be split into partitions for better organization, scalability, and performance. Each message within a partition gets an incremental id, called offset.

   

9. What do you mean by zookeeper in Kafka and what are its uses?

   Zookeeper is used for managing and coordinating Kafka brokers. It's responsible for configuration management, leader detection, and coordination between different nodes in a Kafka cluster. Zookeeper service is used to notify producer and consumer about the presence of any new broker in the Kafka system or failure of any broker, or topic partition occurrence.

   

10. Can we use Kafka without Zookeeper?

    No. Here are a few reasons why Kafka requires ZooKeeper:

    1. Metadata Management: ZooKeeper stores metadata about the Kafka cluster, such as the list of topics, partitions, replica assignments, and their configuration. Kafka brokers query ZooKeeper to obtain this metadata.

    2. Leader Election: ZooKeeper is responsible for electing a leader

       

11. Explain the concept of Leader and Follower in Kafka.

    A Kafka topic is divided into multiple partitions, and each partition can have one leader and multiple followers. The leader is responsible for handling all read and write requests for that partition, while the followers replicate the data from the leader and serve as backups.

    

12. Why is Topic Replication important in Kafka? What do you mean by ISR in Kafka?

    By having multiple replicas (followers) for each partition, Kafka provides fault-tolerance. If a follower fails, it can be easily replaced by another replica. This replication mechanism ensures that data is not lost in case of individual broker failures.

    

13. What do you understand about a consumer group in Kafka?

    ISR stands for "In-Sync Replicas" in Apache Kafka. ISR represents a subset of replicas for a specific partition that are considered in-sync with the leader. In other words, it refers to the set of replicas that have replicated all the messages and are caught up with the leader's log up to a certain point.

    

14. How do you start a Kafka server?

    To start a Kafka server, you need to follow these steps:

    1. Ensure that you have Java installed on your system. Kafka requires Java to run. You can check if Java is installed by running the command `java -version` in your terminal.

    2. Download Kafka: Visit the Apache Kafka website (https://kafka.apache.org/downloads) and download the latest stable release of Kafka. Extract the downloaded archive to a directory of your choice.

    3. Start ZooKeeper: Kafka relies on ZooKeeper for coordination. Open a new terminal window and navigate to the Kafka directory. Run the following command to start ZooKeeper:
       ```
       bin/zookeeper-server-start.sh config/zookeeper.properties
       ```

    4. Start Kafka server: In a separate terminal window, navigate to the Kafka directory again. Run the following command to start the Kafka server:
       ```
       bin/kafka-server-start.sh config/server.properties
       ```

       This command starts the Kafka broker and connects it to the ZooKeeper ensemble.

    5. Verify the server is running: You can check if the Kafka server is running by running the following command in a new terminal window:
       ```
       bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
       ```
       If the server is running, it will display a list of existing topics. If the list is empty, it means there are no topics created yet.

    

15. Tell me about some of the real-world usages of Apache Kafka.

    1. Messaging System: Kafka's high-throughput and fault-tolerant nature make it an excellent choice for building messaging systems. It can handle real-time data streaming and provides reliable message delivery, making it suitable for applications like chat platforms, notifications, and real-time collaboration tools.

    2. Microservices Communication: Kafka facilitates communication and data exchange between microservices in a distributed architecture. It provides a decoupled and scalable communication channel for microservices, enabling asynchronous messaging, event-driven communication, and real-time updates between services.

       

16. Describe partitioning key in Kafka.

    The partitioning key serves two main purposes:

    1. Partition Assignment: The partitioning key is used to determine the partition to which a message will be written. Kafka uses a partitioner algorithm to hash the key and assign the message to a specific partition. By default, Kafka's partitioner algorithm uses a consistent hashing algorithm to distribute messages evenly across partitions based on the key. This ensures that messages with the same key always go to the same partition, providing order and consistency for messages related to a particular key.

    2. Message Ordering: Messages with the same partitioning key are guaranteed to be written to the same partition and are processed in the order they are received. This allows for ordering guarantees within a partition. However, it's important to note that messages with different keys can be written to different partitions and may be processed concurrently.

       

17. What is the purpose of partitions in Kafka?

    1. Scalability: Partitions allow Kafka to handle large amounts of data and support high throughput. A topic can be divided into multiple partitions, and each partition can be distributed across different Kafka brokers. This distribution of data enables Kafka to handle data in parallel, increasing its scalability and the overall throughput of the system.

    2. Parallel Processing: With multiple partitions, Kafka allows for parallel processing of messages. Producers can write to different partitions concurrently, and consumers can read from different partitions in parallel. This parallelism enables efficient and scalable data processing in Kafka.

    3. Fault Tolerance: Partitions provide fault tolerance in Kafka. Each partition has multiple replicas, where one replica acts as the leader and the others act as followers. If a broker or replica fails, one of the followers can be promoted to the new leader, ensuring that the data remains available and the system continues to function. Replication across partitions and brokers ensures data durability and high availability.

    4. Ordering and Consistency: Messages within a partition are strictly ordered. Kafka guarantees the order of messages within a partition, which is important for maintaining the sequence of events or messages related to the same key. This allows Kafka to handle event-driven architectures and stream processing use cases that require ordered message processing.

    5. Load Distribution: Partitions enable load distribution among consumers in Kafka consumer groups. Each consumer in a group is assigned to consume from a subset of partitions. This load distribution allows for horizontal scaling and efficient utilization of resources across consumers, ensuring that the workload is evenly distributed.

    6. Retention and Data Management: Each partition in Kafka has its own configurable retention policy, allowing data to be retained for a specific duration or based on size limits. This enables efficient data management and retention strategies, such as storing data for a certain period or implementing data archival policies.

       

18. Differentiate between Rabbitmq and Kafka.

    **RabbitMQ:**

    - **Design Philosophy**: RabbitMQ is a traditional messaging broker, designed to support complex routing scenarios like point-to-point, request/reply and publish/subscribe. It implements the Advanced Message Queuing Protocol (AMQP) and has plugins supporting the MQ Telemetry Transport (MQTT), Streaming Text Oriented Messaging Protocol (STOMP), and other protocols.
    - **Messaging Model**: RabbitMQ supports both persistent and non-persistent delivery, and allows for flexible routing and message distribution through exchanges.
    - **Ease of Use**: RabbitMQ is known for its ease of installation and configuration. Its support for multiple protocols and its broad plugin ecosystem makes it a flexible choice for many different messaging scenarios.
    - **Performance**: RabbitMQ's performance is high for small messages and where the number of consumers is small or moderate. However, for very large numbers of consumers or very large messages, Kafka tends to be more efficient.
    - **Reliability**: RabbitMQ supports message acknowledgments and publisher confirms, ensuring message reliability.

    **Apache Kafka:**

    - **Design Philosophy**: Kafka was designed to handle real-time, log-based, distributed stream processing. It’s essentially a distributed, partitioned, replicated commit log service.

    - **Messaging Model**: Kafka retains all messages for a set amount of time, and consumers are responsible for tracking their location in each log (offset). Kafka's model allows for massive storage and batch consumption, making it a good fit for big data use cases.

    - **Ease of Use**: Kafka can be complex to set up and manage, due to its distributed nature and configuration parameters.

    - **Performance**: Kafka can handle high-volume real-time message feeds much better than RabbitMQ. It's highly performant for high-throughput use cases, and scales horizontally to support large numbers of consumers.

    - **Reliability**: Kafka is highly available and resilient to node failures. Replication of messages across multiple nodes ensures that no data is lost.

      

19. What are the guarantees that Kafka provides?

    1. Publish-Subscribe Messaging: Kafka follows a publish-subscribe messaging model, where producers publish messages to topics, and consumers can subscribe to those topics to receive messages. Kafka guarantees that all messages published to a topic will be delivered to all interested consumers.

    2. Message Ordering within a Partition: Kafka guarantees that messages within a partition are stored in the order they are received. This ensures strict message ordering within a partition, allowing applications to process messages sequentially and maintain event sequence integrity.

    3. Fault-Tolerance and Durability: Kafka provides fault-tolerance and data durability by replicating data across multiple brokers. Each partition in Kafka has multiple replicas, where one replica acts as the leader and others act as followers. If a broker or replica fails, the leader election process ensures that one of the followers is promoted as the new leader. This ensures that data remains available and the system continues to function even in the presence of failures.

    4. Exactly-Once Semantics: Kafka introduced exactly-once semantics in its newer versions. This ensures that messages are delivered exactly once, eliminating duplicate processing. By leveraging transactional producers and consumers and maintaining proper message offsets, Kafka ensures end-to-end exactly-once semantics for both message production and consumption.

    5. Scalability and High Throughput: Kafka is designed for high scalability and can handle extremely high message throughput. It achieves this by partitioning topics into multiple partitions and distributing them across brokers. This allows for horizontal scalability, parallel processing, and efficient resource utilization.

    6. Data Retention and Persistence: Kafka allows configurable retention policies for data storage. Messages in Kafka topics can be retained for a specific duration or based on size limits. This enables data persistence and historical data retrieval, supporting use cases where data needs to be stored and analyzed over time.

       

20. What do you mean by an unbalanced cluster in Kafka? How can you balance it?

    An unbalanced cluster in Kafka refers to a situation where the distribution of partitions and replicas across the brokers is uneven. This imbalance can occur due to various reasons such as adding or removing brokers, uneven distribution of topic partitions, or failures in the cluster.

    An unbalanced cluster can have negative impacts on the performance, fault-tolerance, and overall efficiency of the Kafka system. It can lead to certain brokers being overloaded with a higher number of partitions, while others remain underutilized. This can result in uneven resource utilization, slower processing, and potential bottlenecks in the system.

    To balance an unbalanced cluster in Kafka, you can take the following steps:

    1. Increase the number of partitions: If certain brokers are overloaded, you can increase the number of partitions for the affected topics. This allows for better distribution of data across brokers and improves parallelism and throughput. However, keep in mind that increasing partitions will require rebalancing and may affect the ordering guarantees within the topic.

    2. Reassign partitions: Kafka provides a mechanism to manually reassign partitions among brokers. You can use the `kafka-reassign-partitions.sh` script to specify the partition reassignment configuration and trigger a rebalance. This helps redistribute partitions across brokers and achieve a more balanced cluster.

    3. Add or remove brokers: If the cluster imbalance is due to the addition or removal of brokers, you can add new brokers or decommission existing ones as needed. Kafka's replication mechanism ensures that partitions are distributed across available brokers, and adding/removing brokers triggers an automatic rebalancing process.

    4. Consider hardware and resource allocation: Evaluate the hardware capacity and resource allocation across brokers in the cluster. Ensure that the brokers have sufficient CPU, memory, and storage resources to handle the expected workload. Adjust the resource allocation and hardware configuration accordingly to achieve a more balanced distribution.

    5. Monitor and optimize: Regularly monitor the cluster's performance and distribution of partitions. Kafka provides several metrics and tools for monitoring cluster health and identifying imbalances. Utilize these tools to analyze the cluster's behavior, identify bottlenecks, and optimize the partition assignment as needed.

       

21. In your recent project, are you a producer or consumer or both?

    Both

    

22. In your recent project, Could you tell me your topic name?

    Orders, shipping, payments

    

23. In your recent project, How many brokers do you have? How many partitions for each topic? How many data for each topic.

    300

    

24. In your recent project, which team produce what kind of event to you and you producer what kind of events?

    Orders team produce payment events to me and we produce shipping events

    

25. What is offset?

​	In Kafka, offset is a pointer to the record in a Kafka topic. It is used to maintain the current position of the consumer.